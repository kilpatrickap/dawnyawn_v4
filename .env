# --- Configuration for Local LLM (e.g., Ollama) ---

# The base URL of your OpenAI-compatible local server
OLLAMA_BASE_URL="http://localhost:11434/v1"

# The API key for the local server (Ollama uses "ollama" by default)
OLLAMA_API_KEY="ollama"

# The exact model name as served by your local API
LLM_MODEL="llama3.1:8b"